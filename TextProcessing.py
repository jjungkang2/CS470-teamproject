# -*- coding: utf-8 -*-
"""Processing_Text_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JE3eHesGb7Cub1ElmTPTvAYirI9Y-g6x

---
# Mount gdrive
"""

from google.colab import drive
drive.mount('/gdrive', force_remount=True)

"""---
# Install libraries
"""

from tensorflow.keras.preprocessing.text import text_to_word_sequence
from nltk.tokenize import TreebankWordTokenizer
from gensim.models import Word2Vec
from torchtext.data import Field
import torch
import numpy
import math

"""---
# Remove outliers and Tokenize
"""

def divide_into_sentences(file_path):
    f = open(file_path, 'r')
    lines = f.readlines()
    f.close()

    clean_lines = []
    punc_to_skip = ['<', '>', '(', ')', '{', '}', '[', ']', '$', '/', '#', '*', '+', '=', '@', '^', '`', '``']
    punc_to_skip += ['à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'ï', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ø', 'ù', 'ú', 'ü', 'ā', 'ć', 'č', 'ē', 'ě', 'ī', 'ū', '˚', '́', 'ย', 'ร', 'อ', '♪', '♫']
    punc_to_skip += ['\x80', '\x93', '\x94', '\xa0', '¡', '¢', '£', '±', '²', 'º', '¿', '–', '—', '‘', '’', '“', '”', '…', '€', '√', '你', '葱', '送']
    punc_to_alp = {'%':' percent', '&':' and', '&amp;':' and', '~':'.', '!':'.', ';':'.', ':':'.', ' --':',', '-':' ', '–':' ', '—':' ', '_':' ', '’':"'"}    
    punc_to_allow = ['okay', 'ok', 'yeah', 'no', 'ok?', 'why?']
    
    for line in lines:
        line = line.strip()
        line = line.lower()
        
        # punc_to_skip이 있으면 스킵
        if len(set(line).intersection(punc_to_skip))>0: continue

        # 대화인지 강조인지 구분을 못해서 삭제
        if ' "' in line: continue
        if '" ' in line: continue
        if " '" in line: continue
        if "' " in line: continue
        if "''" in line: continue
        
        # punc_to_alp로 변환
        for punc in punc_to_alp:
            line = line.replace(punc, punc_to_alp[punc])

        line = line.strip()
        if len(line)<3: continue

        # 문장 분리
        if '.' in line:
            line = line.split('.')
            line = [l.strip() for l in line if len(l.strip())>5 or l.strip().isdigit() or l.strip() in punc_to_allow]
            clean_lines.extend(line)
        else:
            clean_lines.append(line)

    return clean_lines

root = 'input/'
train_data = divide_into_sentences(root+'train_source.txt')
test_data = divide_into_sentences(root+'test_source.txt')

tokenizer = TreebankWordTokenizer()

train_input_raw = []
train_output_raw = []

for sent in train_data:
    sent = tokenizer.tokenize(sent)

    remove_end = [',', "''", "'"]
    remove_start = ["''", ',', '?']
    if sent[-1] in remove_end: del sent[-1]
    if sent[0] in remove_start: del sent[0]
    if sent[-1] in remove_end: del sent[-1]
    if sent[0] in remove_start: del sent[0]

    punc = [1] * len(sent)
    punc[-1] = 2

    # 0: padding
    # 1: space
    # 2: .
    # 3: ,
    # 4: ?
    for ind, word in enumerate(sent):
        if word==',': punc[ind-1]=3
        if word=='?': punc[ind-1]=4
    
    for ind in reversed(range(len(punc))):
        if punc[ind]==3 or punc[ind]==4:    
            del sent[ind+1]
            del punc[ind+1]
    
    train_input_raw.append(sent)
    train_output_raw.append(punc)

test_input_raw = []
test_output_raw = []

for sent in test_data:
    sent = tokenizer.tokenize(sent)

    remove_end = [',', "''", "'"]
    remove_start = ["''", ',', '?']
    if sent[-1] in remove_end: del sent[-1]
    if sent[0] in remove_start: del sent[0]
    if sent[-1] in remove_end: del sent[-1]
    if sent[0] in remove_start: del sent[0]

    punc = [1] * len(sent)
    punc[-1] = 2

    for ind, word in enumerate(sent):
        if word==',': punc[ind-1]=3
        if word=='?': punc[ind-1]=4
    
    for ind in reversed(range(len(punc))):
        if punc[ind]==3 or punc[ind]==4:    
            del sent[ind+1]
            del punc[ind+1]
    
    test_input_raw.append(sent)
    test_output_raw.append(punc)

print(test_input_raw[0])
print(test_output_raw[0])

"""---
# Make WordDict
"""

TEXT = Field()
TEXT.build_vocab(train_input_raw, min_freq=10)
TEXT.vocab.stoi['<unk>'] = 1
TEXT.vocab.stoi['<pad>'] = 0

print(TEXT.vocab.stoi)

"""---
# Divide by 100 and Use Padding
"""

train_input = []
train_output = []

train_input_extended = []
train_output_extended = []

for train_inputs in train_input_raw:
    for words in train_inputs:
        train_input_extended.append(TEXT.vocab.stoi[words])
for train_outputs in train_output_raw:
    train_output_extended.extend(train_outputs)

len_with_pad = math.ceil(len(train_input_extended)/100)*100

train_input_extended.extend(["<pad>"]*(len_with_pad - len(train_input_extended)))
train_output_extended.extend([0]*(len_with_pad - len(train_output_extended)))

for i in range(len_with_pad//100):
    train_input.append(train_input_extended[i*100:(i+1)*100])
    train_output.append(train_output_extended[i*100:(i+1)*100])

print(len(train_input_extended))
print(len(train_input))
print(train_input[0])
print(train_output[0])

test_input = []
test_output = []

test_input_extended = []
test_output_extended = []

for test_inputs in test_input_raw:
    for words in test_inputs:
        try:
            num = TEXT.vocab.stoi[words]
        except:
            num = 1
        test_input_extended.append(num)
for test_outputs in test_output_raw:
    test_output_extended.extend(test_outputs)

len_with_pad = math.ceil(len(test_input_extended)/100)*100

test_input_extended.extend(["<pad>"]*(len_with_pad - len(test_input_extended)))
test_output_extended.extend([0]*(len_with_pad - len(test_output_extended)))

for i in range(len_with_pad//100):
    test_input.append(test_input_extended[i*100:(i+1)*100])
    test_output.append(test_output_extended[i*100:(i+1)*100])

print(len(test_input_extended))
print(len(test_input))
print(test_input[0])
print(test_output[0])

print(len(train_input), len(train_output))
print(train_input[0])
print(train_output[0])
print(len(test_input), len(test_output))
print(test_input[0])
print(test_output[0])
print(TEXT.vocab.stoi)

import pickle

with open(root + 'train_input', 'wb') as f:
    pickle.dump(train_input, f, pickle.HIGHEST_PROTOCOL)

with open(root + 'train_output', 'wb') as f:
    pickle.dump(train_output, f, pickle.HIGHEST_PROTOCOL)

with open(root + 'test_input', 'wb') as f:
    pickle.dump(test_input, f, pickle.HIGHEST_PROTOCOL)

with open(root + 'test_output', 'wb') as f:
    pickle.dump(test_output, f, pickle.HIGHEST_PROTOCOL)

with open(root + 'word_dict', 'wb') as f:
    pickle.dump(TEXT.vocab.stoi, f, pickle.HIGHEST_PROTOCOL)